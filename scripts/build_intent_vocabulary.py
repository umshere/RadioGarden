#!/usr/bin/env python3
import json
import os
import sys
from collections import Counter, defaultdict

import requests

BASE_URL = os.environ.get("RADIO_PASSPORT_BASE_URL", "http://localhost:5173")
CATALOG_ENDPOINT = f"{BASE_URL}/api/radio-catalog"
OUTPUT_FILE = "app/services/ai/intent/generatedVocabulary.ts"

TOP_COUNTRIES = 200
TOP_LANGUAGES = 200
TOP_TAGS = 400

ALIASES = {
    "usa": ["united states", "america", "american", "us"],
    "uk": ["united kingdom", "britain", "england", "scotland", "wales"],
    "uae": ["united arab emirates", "dubai", "abu dhabi"],
    "south korea": ["korea", "korean", "sk"],
    "dr congo": ["democratic republic of the congo", "drc"],
}

LANGUAGE_ALIASES = {
    "hindi": ["hindustani", "bollywood"],
    "tamil": ["tamizh"],
    "malayalam": ["malayalee"],
    "telugu": ["telegu"],
    "punjabi": ["punjabi", "punjab"],
    "spanish": ["espanol"],
    "portuguese": ["portugues"],
    "french": ["francais"],
}

def title_case(name: str) -> str:
    parts = name.split()
    return " ".join(part.capitalize() for part in parts)

def fetch_catalog():
    try:
        res = requests.get(CATALOG_ENDPOINT, timeout=30)
        res.raise_for_status()
        return res.json()
    except requests.RequestException as exc:
        print(f"Failed to fetch catalog: {exc}", file=sys.stderr)
        sys.exit(1)

def aggregate_aliases(items, key, limit):
    counter = Counter()
    for station in items:
        value = station.get(key)
        if not value:
            continue
        counter[value.strip().lower()] += 1
    top = counter.most_common(limit)
    return top

def build_country_map(stations, countries):
    country_counts = aggregate_aliases(stations, "country", TOP_COUNTRIES)
    country_map = defaultdict(set)

    for name, _ in country_counts:
        canonical = title_case(name)
        country_map[canonical].add(name)

    for country in countries:
        raw = country.get("name")
        if raw:
            canonical = title_case(raw)
            country_map[canonical].add(raw.lower())

    for canonical, extras in ALIASES.items():
        title = title_case(canonical)
        for alias in extras:
            country_map[title].add(alias.lower())

    return {
        country: sorted(list(aliases))
        for country, aliases in sorted(country_map.items())
    }

def build_language_map(languages):
    language_map = defaultdict(set)
    for lang in languages:
        name = lang.get("name")
        if not name:
            continue
        canonical = title_case(name)
        language_map[canonical].add(name.lower())

    for canonical, aliases in LANGUAGE_ALIASES.items():
        title = title_case(canonical)
        for alias in aliases:
            language_map[title].add(alias.lower())

    return {
        language: sorted(list(aliases))
        for language, aliases in sorted(language_map.items())
    }

def build_tag_map(stations):
    counter = Counter()
    for station in stations:
        tags = station.get("tagList")
        if not tags:
            continue
        for tag in tags:
            if tag:
                counter[tag.strip().lower()] += 1

    top_tags = counter.most_common(TOP_TAGS)
    tag_map = {}
    for tag, _ in top_tags:
        canonical = tag.lower()
        synonyms = [canonical]
        if " / " in canonical:
            synonyms.extend(part.strip() for part in canonical.split("/"))
        if "-" in canonical:
            synonyms.extend(part.strip() for part in canonical.split("-"))
        tag_map[tag] = sorted(set(filter(None, synonyms)))

    return tag_map

def write_typescript(data):
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("// Auto-generated by scripts/build_intent_vocabulary.py\n")
        f.write("// DO NOT EDIT MANUALLY\n\n")
        json_blob = json.dumps(data, indent=2, ensure_ascii=False)
        f.write("export const GENERATED_INTENT_VOCAB = ")
        f.write(json_blob)
        f.write(";\n")


def main():
    catalog = fetch_catalog()
    stations = catalog.get("stations", [])
    countries = catalog.get("countries", [])
    languages = catalog.get("languages", [])

    country_map = build_country_map(stations, countries)
    language_map = build_language_map(languages)
    tag_map = build_tag_map(stations)

    data = {
        "fetchedAt": catalog.get("fetchedAt"),
        "countries": country_map,
        "languages": language_map,
        "tags": tag_map,
    }

    write_typescript(data)
    print(
        f"Generated vocabulary with {len(country_map)} countries, {len(language_map)} languages, {len(tag_map)} tags -> {OUTPUT_FILE}"
    )

if __name__ == "__main__":
    main()
